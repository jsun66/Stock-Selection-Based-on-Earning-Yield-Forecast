# Background and Purpose
Long-term investors, different from short-term traders, focus on examining the underlying forces that affect the well-being of a company. They rely on fundamental analysis which attempts to measure the intrinsic value an equity. Quantitative investment researchers have identified some value factors to determine the cost of investment for a stock and compare different stocks. This paper proposes using sequence prediction models to forecast a value factor-the earning yield (EBIT/EV) of a company for stock selection. Two advanced sequence prediction models-Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) networks are studied. These two models can overcome the inherent problems of a standard Recurrent Neural Network, i.e., vanishing and exploding gradients.
# Modeling Method
This project used a method that predicts company’s future EBIT/EV using sequence prediction models and invests in the companies with the highest rankings of EBIT/EV. In this paper, two sequence prediction models are applied. They are Long Short-term Memory (LSTM) Network and Gated Recurrent Unit (GRU) Network. They are also compared to the traditional non-sequential Feed-forward Neural Network (FNN). LSTM and GRU networks are special types of recurrent neural networks (RNNs). A RNN can be viewed as multiple FNNs where one FNN is connected to the next time step FNN through the hidden neurons. This makes RNN a powerful type of neural network to handle sequence dependence modeling such as time series prediction which forecasts future values according to previously observed values. Forecasting company’s future EBIT/EV based on previously reported fundamental data is a time series prediction problem. The workflow of the proposed modeling method is shown in Fig.1.

![Alt text](https://github.com/jsun66/Stock-Selection-Based-on-Earning-Yield-Forecast/blob/main/Tables%20and%20Figures/Fig.1.%20Workflow.jpg)
### LSTM Network
LSTM networks are designed specifically to learn the long-term dependencies. Like a simple FNN, a LSTM network is also consisted of an input layer, one or more hidden layers and an output layer. The main characteristics of LSTM networks is contained in the hidden layers of which the values are now determined by special units known as memory cells. Memory cells are sophisticatedly designed to control the information flow from one time step to the next. The structure of a memory cell is illustrated in Fig.2.

![Alt text](https://github.com/jsun66/Stock-Selection-Based-on-Earning-Yield-Forecast/blob/main/Tables%20and%20Figures/Fig.2.%20LSTM.jpg)
### GRU Network
The GRU network is a newer sequence prediction model proposed in the work of J. Chung and Bengio. It is also designed to solve the vanishing and exploding gradients problems of standard RNNs. The GRU unit has an architecture similar to a LSTM unit while the differences are: First, GRU eliminates the use of memory cells and the information flow is carried by the hidden states. Second, GRU merges the forget gate and the input gate into one update gate. The structure of a GRU unit is shown in Fig.3.

![Alt text](https://github.com/jsun66/Stock-Selection-Based-on-Earning-Yield-Forecast/blob/main/Tables%20and%20Figures/Fig.3.%20GRU.jpg)
# Data Structuring
Since many companies’ fundamentals are fed into the models simultaneously and companies of different sizes usually have very different values of fundamental account variables, for normalization purpose, all raw input fundamental variables are divided by the EV of the company. As shown in Table I, the data is then constructed into a set of sequences where each sequence represents the change of acompany through a period of time. The amount of high-quality data that one can currently get on broad-market company fundamentals spans about 58 years (from 1960 to 2018). It would give us 232 independent time periods (quarters) which is sufficient for learning and the time horizon is long enough not to be influenced by exogenous factors. Data is fed into the model in time steps spaced by a one-quarter interval. 14 fundamentals in two years (112 features, 8 quarters) are used to predict the company’s EBIT/EV in the next quarter.

![Alt text](https://github.com/jsun66/Stock-Selection-Based-on-Earning-Yield-Forecast/blob/main/Tables%20and%20Figures/Table%20I.PNG)
# Network Hyperparameter Setup and Optimization
A network’s hyperparameter is a parameter of which the value is set before the training process while other parameters of the network are determined through training. In other words, a model learns the parameters from the data under certain hyperparameters. For a neural network, there are two types of hyperparameters. One is to determine the network structure (e.g. number of hidden layers) and the other one is to determine how the network is trained (e.g. learning rate). The traditional way to determine these hyperparameters is known as grid search, which is to search exhaustively through a manually specified finite set of reasonable values. For example, the number of hidden layers ∈ {1,2,3} and the number of neurons ∈ {40,50,60} generates 9 combinations in total. Grid search goes through all these 9 combinations and outputs the one with the best performance. A list of the optimized hyperparameters used for this study is shown in Table II.

![Alt text](https://github.com/jsun66/Stock-Selection-Based-on-Earning-Yield-Forecast/blob/main/Tables%20and%20Figures/Table%20II.PNG)
# Results
The results of the three models’ MAPEs are shown in below table:

![Alt text](https://github.com/jsun66/Stock-Selection-Based-on-Earning-Yield-Forecast/blob/main/Tables%20and%20Figures/Table%20III.PNG)

Both LSTM and GRU models demonstrate superior performance of forecast accuracy compared to traditional FNN model. Although in this application the MAPE result shows the GRU model slightly outperformed the LSTM model, it is not safe to draw the conclusion that the GRU model is always better than the LSTM model from the precision perspective. In practice, it is suggested using a trial and error approach to choose between LSTM and GRU models for a specific problem with a specific dataset.
